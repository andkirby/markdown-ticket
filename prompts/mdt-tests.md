# MDT Test Specification Workflow (v1)

Generate BDD test specifications and executable test files from requirements or behavioral assessment.

**Core Principle**: Tests are written BEFORE implementation. They define success criteria, not verify after-the-fact.

## User Input

```text
$ARGUMENTS
```

## Output Location

- Specification: `docs/CRs/{CR-KEY}/tests.md`
- Test files: Project's test directory (detected from config)

## Mode Detection

| CR Type | Input Source | Test Strategy |
|---------|--------------|---------------|
| Feature | requirements.md | Behavior specification â€” test what SHOULD happen |
| Refactoring | assess output + existing code | Behavior preservation â€” test what CURRENTLY happens |
| Tech Debt | assess output + existing code | Behavior preservation â€” lock before changing |

## Critical Rules

1. **Tests before code** â€” Generate failing tests, implementation makes them pass
2. **Integration/E2E focus** â€” Test from public interface, not internals
3. **BDD format** â€” Given/When/Then scenarios derived from EARS specs
4. **Traceability** â€” Every test traces to requirement or behavior

## Execution Steps

### Step 1: Detect Mode and Load Context

**1a. Load CR:**
```
mdt-all:get_cr mode="full"
```

**1b. Determine mode:**

```
IF docs/CRs/{CR-KEY}/requirements.md exists:
  MODE = "feature"
  Load requirements.md
ELSE IF CR type is "Bug Fix" or contains refactoring keywords:
  MODE = "refactoring"  
  Load assess output if exists
ELSE:
  MODE = "feature"
  Warn: "No requirements.md found â€” generate from CR description"
```

**1c. Detect test framework:**

From project config (package.json, pyproject.toml, Cargo.toml, etc.):

| Language | Common Frameworks | Detection |
|----------|-------------------|-----------|
| TypeScript/JS | Jest, Vitest, Mocha | package.json devDependencies |
| Python | Pytest, unittest | pyproject.toml, requirements-dev.txt |
| Rust | built-in, rstest | Cargo.toml dev-dependencies |
| Go | testing, testify | go.mod, _test.go patterns |

```yaml
test:
  framework: {jest, pytest, etc.}
  directory: {tests/, __tests__/, test/, etc.}
  pattern: {*.test.ts, test_*.py, *_test.go, etc.}
  command: {npm test, pytest, cargo test, go test}
```

### Step 2: Extract Test Subjects

**For Feature Mode (from requirements.md):**

Parse EARS specifications:

| EARS Type | Maps To |
|-----------|---------|
| WHEN `<trigger>` THEN `<response>` | Happy path scenario |
| IF `<condition>` THEN `<response>` | Conditional scenario |
| WHILE `<state>` the system shall | State-based scenario |
| WHERE `<feature>` is not available | Fallback scenario |

Example extraction:
```markdown
## R1.1
WHEN valid URL is provided THEN the system shall extract summary points

â†’ Scenario: successful_extraction
  Given: valid article URL
  When: summarize command invoked
  Then: returns non-empty points array
```

**For Refactoring Mode (from code analysis):**

Identify current behaviors to preserve:

1. Public function signatures
2. Return value shapes
3. Error conditions and codes
4. Side effects (file writes, API calls, events)
5. Edge case handling

```markdown
## Behavior: getUser()
Currently returns: { id, name, email } or throws NotFoundError

â†’ Scenario: preserve_user_shape
  Given: valid user ID
  When: getUser called
  Then: returns object with id, name, email fields

â†’ Scenario: preserve_not_found_error  
  Given: non-existent user ID
  When: getUser called
  Then: throws NotFoundError (not generic Error)
```

### Step 3: Generate BDD Scenarios

**Scenario Template:**

```gherkin
Feature: {Feature name from requirement group}

  Background:
    Given {common setup}

  @requirement:{R-ID}
  Scenario: {descriptive_name}
    Given {initial context}
    When {action taken}
    Then {expected outcome}
    And {additional assertions}

  @requirement:{R-ID}
  Scenario: {edge_case_name}
    Given {edge condition}
    When {action taken}
    Then {expected handling}
```

**Coverage Requirements:**

| Requirement Type | Minimum Scenarios |
|------------------|-------------------|
| Happy path (WHEN...THEN) | 1 success + 1 variation |
| Conditional (IF...THEN) | 1 per condition branch |
| Error handling | 1 per error type |
| State-based (WHILE) | Entry, during, exit states |

### Step 4: Generate Executable Test Files

**4a. Create test file structure:**

```
{test_directory}/
â””â”€â”€ {CR-KEY}/                    # or integrated into existing structure
    â”œâ”€â”€ {feature-a}.test.{ext}
    â”œâ”€â”€ {feature-b}.test.{ext}
    â””â”€â”€ helpers/
        â””â”€â”€ fixtures.{ext}
```

**4b. Generate test code (framework-specific):**

**Jest/Vitest (TypeScript):**
```typescript
/**
 * Tests for: {CR-KEY}
 * Requirements: {R1.1, R1.2, ...}
 * Generated by: /mdt:tests
 * Status: RED (implementation pending)
 */

describe('{Feature name}', () => {
  // @requirement: R1.1
  describe('when valid URL provided', () => {
    it('should extract summary points', async () => {
      // Arrange
      const url = 'https://example.com/article';
      
      // Act
      const result = await summarize(url);
      
      // Assert
      expect(result.points).toBeInstanceOf(Array);
      expect(result.points.length).toBeGreaterThan(0);
      expect(result.points[0]).toHaveProperty('text');
    });

    it('should include source metadata', async () => {
      const url = 'https://example.com/article';
      const result = await summarize(url);
      
      expect(result.source).toBe(url);
      expect(result.extractedAt).toBeInstanceOf(Date);
    });
  });

  // @requirement: R1.2
  describe('when URL times out', () => {
    it('should retry up to 3 times', async () => {
      const slowUrl = 'https://slow.example.com';
      const startTime = Date.now();
      
      await expect(summarize(slowUrl)).rejects.toThrow('timeout');
      
      // Verify retries occurred (rough timing check)
      const elapsed = Date.now() - startTime;
      expect(elapsed).toBeGreaterThan(3000); // 3 retries
    });
  });
});
```

**Pytest (Python):**
```python
"""
Tests for: {CR-KEY}
Requirements: {R1.1, R1.2, ...}
Generated by: /mdt:tests
Status: RED (implementation pending)
"""
import pytest
from src.commands.summarize import summarize


class TestSummarize:
    """Feature: Summarize Command"""

    # @requirement: R1.1
    class TestValidUrl:
        """when valid URL provided"""

        async def test_extracts_summary_points(self):
            """should extract summary points"""
            # Arrange
            url = "https://example.com/article"

            # Act
            result = await summarize(url)

            # Assert
            assert isinstance(result.points, list)
            assert len(result.points) > 0
            assert "text" in result.points[0]

        async def test_includes_source_metadata(self):
            """should include source metadata"""
            url = "https://example.com/article"
            result = await summarize(url)

            assert result.source == url
            assert result.extracted_at is not None

    # @requirement: R1.2
    class TestTimeout:
        """when URL times out"""

        async def test_retries_three_times(self):
            """should retry up to 3 times"""
            slow_url = "https://slow.example.com"

            with pytest.raises(TimeoutError):
                await summarize(slow_url)
```

**4c. Integration test patterns:**

For E2E/integration focus, test from the outside:

```typescript
// CLI integration test
describe('summarize CLI', () => {
  it('outputs points to stdout', async () => {
    const { stdout, exitCode } = await exec('mycli summarize https://example.com');
    
    expect(exitCode).toBe(0);
    expect(stdout).toContain('Points:');
  });
});

// API integration test  
describe('POST /api/summarize', () => {
  it('returns points in response body', async () => {
    const response = await request(app)
      .post('/api/summarize')
      .send({ url: 'https://example.com' });
    
    expect(response.status).toBe(200);
    expect(response.body.points).toBeInstanceOf(Array);
  });
});
```

### Step 5: Verify Tests are RED

After generating test files:

```bash
{test_command} --filter={CR-KEY}
```

Expected output:
```
Tests: X failed, 0 passed
```

If any tests pass before implementation â†’ investigate:
- Is there existing code that satisfies this?
- Is the test too loose?
- Is this a duplicate requirement?

### Step 6: Generate tests.md

```markdown
# Tests: {CR-KEY}

**Mode**: {Feature | Refactoring}
**Source**: {requirements.md | assess + code analysis}
**Generated**: {timestamp}

## Test Configuration

| Setting | Value |
|---------|-------|
| Framework | {jest, pytest, etc.} |
| Test directory | `{path}` |
| Test command | `{command}` |
| CR test filter | `--filter={CR-KEY}` |

## Requirement â†’ Test Mapping

| Req ID | Description | Test File | Scenarios | Status |
|--------|-------------|-----------|-----------|--------|
| R1.1 | Extract points from URL | `summarize.test.ts` | 2 | ðŸ”´ RED |
| R1.2 | Retry on timeout | `summarize.test.ts` | 1 | ðŸ”´ RED |
| R2.1 | Show progress | `progress.test.ts` | 2 | ðŸ”´ RED |

## Test Specifications

### Feature: {Feature Name}

**File**: `{test_directory}/{feature}.test.{ext}`
**Covers**: R1.1, R1.2

#### Scenario: successful_extraction (R1.1)

```gherkin
Given a valid article URL
When summarize command is invoked
Then it should return extracted points
And points array should be non-empty
And each point should have text property
```

**Test**: `describe('when valid URL') > it('should extract summary points')`

#### Scenario: timeout_retry (R1.2)

```gherkin
Given a slow-responding URL
When request exceeds timeout threshold
Then it should retry up to 3 times
And if all retries fail, throw TimeoutError
```

**Test**: `describe('when URL times out') > it('should retry up to 3 times')`

---

### Feature: {Feature Name 2}

**File**: `{test_directory}/{feature2}.test.{ext}`
**Covers**: R2.1

...

---

## Edge Cases

| Scenario | Expected Behavior | Test | Req |
|----------|-------------------|------|-----|
| Malformed URL | ValidationError with message | `invalid_url.test.ts` | R1.1 |
| Empty response body | EmptyContentError | `edge_cases.test.ts` | R1.1 |
| Rate limited (429) | Backoff and retry | `edge_cases.test.ts` | R1.2 |
| Network offline | NetworkError, no retry | `edge_cases.test.ts` | R1.2 |

## Generated Test Files

| File | Scenarios | Lines | Status |
|------|-----------|-------|--------|
| `{test_dir}/summarize.test.ts` | 5 | ~80 | ðŸ”´ RED |
| `{test_dir}/progress.test.ts` | 2 | ~40 | ðŸ”´ RED |
| `{test_dir}/edge_cases.test.ts` | 4 | ~60 | ðŸ”´ RED |

## Verification

Run tests (should all fail):
```bash
{test_command}
```

Expected: **{N} failed, 0 passed**

## Coverage Checklist

- [x] All requirements have at least one test
- [x] Error scenarios covered
- [x] Edge cases from architecture included
- [ ] Tests are RED (verified manually)

---

## For Implementation

Each task in `/mdt:tasks` should reference which tests it will make GREEN:

| Task | Makes GREEN |
|------|-------------|
| Task 2.1 | `summarize.test.ts` (R1.1) |
| Task 2.2 | `summarize.test.ts` (R1.2) |
| Task 3.1 | `progress.test.ts` (R2.1) |

After each task: `{test_command}` should show fewer failures.
```

### Step 7: Save and Report

**7a. Save test files** to project test directory

**7b. Save tests.md** to `docs/CRs/{CR-KEY}/tests.md`

**7c. Report:**

```markdown
## Tests Generated: {CR-KEY}

| Metric | Value |
|--------|-------|
| Mode | {Feature / Refactoring} |
| Requirements covered | {N}/{M} |
| Test files created | {N} |
| Total scenarios | {N} |
| Status | ðŸ”´ All RED |

**Test files**:
- `{path/to/test1.test.ext}`
- `{path/to/test2.test.ext}`

**Verify RED**:
```bash
{test_command}
# Expected: {N} failed, 0 passed
```

**Next**: `/mdt:tasks {CR-KEY}` â€” tasks will reference which tests to make GREEN
```

---

## Refactoring Mode Details

When MODE = "refactoring", test generation differs:

### Step 2 (Refactoring): Analyze Current Behavior

```markdown
## Behavioral Analysis: {file}

### Public Interface

| Export | Signature | Current Behavior |
|--------|-----------|------------------|
| `getUser` | `(id: string) => Promise<User>` | Returns user or throws NotFoundError |
| `updateUser` | `(id: string, data: Partial<User>) => Promise<User>` | Merges data, returns updated |

### Error Contracts

| Function | Error Type | Condition |
|----------|------------|-----------|
| `getUser` | `NotFoundError` | User ID doesn't exist |
| `getUser` | `ValidationError` | ID format invalid |
| `updateUser` | `NotFoundError` | User ID doesn't exist |
| `updateUser` | `ConflictError` | Concurrent modification |

### Side Effects

| Function | Side Effect | Observable Via |
|----------|-------------|----------------|
| `updateUser` | Emits 'user.updated' event | Event listener |
| `updateUser` | Updates `modified_at` timestamp | Return value |

### Discovered Behaviors (undocumented)

From code analysis:
- `getUser` caches results for 5 minutes
- `updateUser` silently ignores unknown fields
- Empty string ID treated as null (returns first user?!)
```

### Step 4 (Refactoring): Preservation Tests

```typescript
/**
 * Behavioral Preservation Tests
 * Source: Code analysis of src/users/index.ts
 * Purpose: Lock current behavior before refactoring
 * 
 * âš ï¸ These tests document CURRENT behavior, not DESIRED behavior.
 * If a test seems wrong, that's a bug to fix AFTER refactoring.
 */

describe('getUser - behavioral preservation', () => {
  it('returns user object with expected shape', async () => {
    const user = await getUser('user-123');
    
    // Lock the return shape
    expect(user).toMatchObject({
      id: expect.any(String),
      name: expect.any(String),
      email: expect.any(String),
    });
  });

  it('throws NotFoundError (not generic Error) for missing user', async () => {
    await expect(getUser('nonexistent'))
      .rejects
      .toBeInstanceOf(NotFoundError); // Not just Error
  });

  // Discovered behavior - may be bug, but lock it for now
  it('treats empty string ID as null (returns first user)', async () => {
    const user = await getUser('');
    expect(user).toBeDefined(); // Current behavior
    // TODO: After refactoring, consider if this should throw
  });
});
```

---

## Integration with Downstream Prompts

### `/mdt:tasks` Receives

```markdown
## Test Context (from tests.md)

| Test File | Scenarios | Status |
|-----------|-----------|--------|
| `summarize.test.ts` | 5 | ðŸ”´ RED |

### Task â†’ Test Mapping

Task 2.1 (Extract summarize command) should make GREEN:
- `summarize.test.ts > when valid URL > should extract summary points`
- `summarize.test.ts > when valid URL > should include source metadata`
```

### `/mdt:implement` Verifies

```markdown
## TDD Verification (Step 3d)

Before task: Run `{test_command} --filter={relevant_tests}`
- Record which tests are RED

After task: Run `{test_command} --filter={relevant_tests}`
- Verify expected tests are now GREEN
- No previously GREEN tests became RED (regression)
```

---

## Examples

### Feature Mode Example

Input (requirements.md):
```markdown
## R1.1
WHEN user provides valid URL THEN system shall return summary points

## R1.2  
IF request times out THEN system shall retry up to 3 times
```

Output (test file):
```typescript
describe('summarize', () => {
  // @requirement: R1.1
  it('returns summary points for valid URL', async () => {
    const result = await summarize('https://example.com');
    expect(result.points).toBeInstanceOf(Array);
    expect(result.points.length).toBeGreaterThan(0);
  });

  // @requirement: R1.2
  it('retries up to 3 times on timeout', async () => {
    // ... timeout test
  });
});
```

### Refactoring Mode Example

Input (from assess):
```markdown
## File: src/cli/index.ts
Test Coverage: 23% ðŸ”´ Critical
Behaviors needing tests: URL validation, error codes, retry logic
```

Output (test file):
```typescript
/**
 * Behavioral Preservation: src/cli/index.ts
 * Lock current behavior before refactoring
 */
describe('CLI behavioral preservation', () => {
  it('validates URL format before processing', async () => {
    const { exitCode, stderr } = await exec('mycli summarize not-a-url');
    expect(exitCode).toBe(1);
    expect(stderr).toContain('Invalid URL');
  });

  it('returns exit code 2 for network errors', async () => {
    // ... network error test
  });
});
```

---

## Validation Checklist

Before completing `/mdt:tests`:

- [ ] Mode correctly detected (feature vs refactoring)
- [ ] Test framework detected from project config
- [ ] All requirements/behaviors have test coverage
- [ ] BDD scenarios in Gherkin format
- [ ] Executable test files generated
- [ ] Tests verified as RED
- [ ] tests.md includes requirement mapping
- [ ] Task â†’ test mapping prepared for `/mdt:tasks`

Context: $ARGUMENTS
